[2025-10-09T06:11:45.987+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-10-09T06:11:46.035+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_analytics.run_spark_stream scheduled__2025-10-09T04:00:00+00:00 [queued]>
[2025-10-09T06:11:46.049+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_analytics.run_spark_stream scheduled__2025-10-09T04:00:00+00:00 [queued]>
[2025-10-09T06:11:46.060+0000] {taskinstance.py:2306} INFO - Starting attempt 3 of 4
[2025-10-09T06:11:46.089+0000] {taskinstance.py:2330} INFO - Executing <Task(BashOperator): run_spark_stream> on 2025-10-09 04:00:00+00:00
[2025-10-09T06:11:46.105+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:62: DeprecationWarning: This process (pid=717) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-10-09T06:11:46.108+0000] {standard_task_runner.py:64} INFO - Started process 807 to run task
[2025-10-09T06:11:46.109+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'restaurant_analytics', 'run_spark_stream', 'scheduled__2025-10-09T04:00:00+00:00', '--job-id', '249', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_dag.py', '--cfg-path', '/tmp/tmp9mg0w60t']
[2025-10-09T06:11:46.116+0000] {standard_task_runner.py:91} INFO - Job 249: Subtask run_spark_stream
[2025-10-09T06:11:46.294+0000] {task_command.py:426} INFO - Running <TaskInstance: restaurant_analytics.run_spark_stream scheduled__2025-10-09T04:00:00+00:00 [running]> on host 01b28d042a8a
[2025-10-09T06:11:46.536+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='quantruong1918@gmail.com' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='restaurant_analytics' AIRFLOW_CTX_TASK_ID='run_spark_stream' AIRFLOW_CTX_EXECUTION_DATE='2025-10-09T04:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-10-09T04:00:00+00:00'
[2025-10-09T06:11:46.540+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-10-09T06:11:46.542+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-10-09T06:11:46.544+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'export PYSPARK_PYTHON=/usr/local/bin/python && export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python && /opt/spark/bin/spark-submit --master local[*] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 /opt/***/dags/spark_streaming.py']
[2025-10-09T06:11:46.575+0000] {subprocess.py:86} INFO - Output:
[2025-10-09T06:11:51.164+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-10-09T06:11:51.364+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-10-09T06:11:51.366+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-10-09T06:11:51.383+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-10-09T06:11:51.386+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-0efcaae7-3d86-4e5b-a319-20dd90c76266;1.0
[2025-10-09T06:11:51.388+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-10-09T06:11:51.883+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[2025-10-09T06:11:52.073+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[2025-10-09T06:11:52.230+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-10-09T06:11:52.329+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-10-09T06:11:52.395+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-10-09T06:11:52.453+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-10-09T06:11:52.553+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-10-09T06:11:52.650+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-10-09T06:11:52.721+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-10-09T06:11:52.765+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-10-09T06:11:52.819+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-10-09T06:11:52.923+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 1480ms :: artifacts dl 58ms
[2025-10-09T06:11:52.924+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-10-09T06:11:52.925+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-10-09T06:11:52.926+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-10-09T06:11:52.927+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-10-09T06:11:52.928+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-10-09T06:11:52.929+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-10-09T06:11:52.930+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-10-09T06:11:52.931+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
[2025-10-09T06:11:52.931+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
[2025-10-09T06:11:52.932+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-10-09T06:11:52.933+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-10-09T06:11:52.934+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-10-09T06:11:52.935+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-10-09T06:11:52.936+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-10-09T06:11:52.936+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-10-09T06:11:52.938+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-10-09T06:11:52.943+0000] {subprocess.py:93} INFO - 	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
[2025-10-09T06:11:52.944+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-10-09T06:11:52.945+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-0efcaae7-3d86-4e5b-a319-20dd90c76266
[2025-10-09T06:11:52.945+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-10-09T06:11:52.956+0000] {subprocess.py:93} INFO - 	0 artifacts copied, 11 already retrieved (0kB/20ms)
[2025-10-09T06:11:53.514+0000] {subprocess.py:93} INFO - 25/10/09 06:11:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-09T06:12:01.011+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SparkContext: Running Spark version 3.5.1
[2025-10-09T06:12:01.013+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SparkContext: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-10-09T06:12:01.014+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SparkContext: Java version 17.0.16
[2025-10-09T06:12:01.059+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceUtils: ==============================================================
[2025-10-09T06:12:01.060+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-09T06:12:01.061+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceUtils: ==============================================================
[2025-10-09T06:12:01.062+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SparkContext: Submitted application: RestaurantStreaming
[2025-10-09T06:12:01.105+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-09T06:12:01.115+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceProfile: Limiting resource is cpu
[2025-10-09T06:12:01.117+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-09T06:12:01.209+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SecurityManager: Changing view acls to: ***
[2025-10-09T06:12:01.216+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SecurityManager: Changing modify acls to: ***
[2025-10-09T06:12:01.219+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SecurityManager: Changing view acls groups to:
[2025-10-09T06:12:01.220+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SecurityManager: Changing modify acls groups to:
[2025-10-09T06:12:01.221+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-10-09T06:12:01.963+0000] {subprocess.py:93} INFO - 25/10/09 06:12:01 INFO Utils: Successfully started service 'sparkDriver' on port 43671.
[2025-10-09T06:12:02.109+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO SparkEnv: Registering MapOutputTracker
[2025-10-09T06:12:02.209+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-09T06:12:02.259+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-09T06:12:02.261+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-09T06:12:02.279+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-09T06:12:02.363+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8dd612b2-84f6-491c-aed3-9f49cd15be27
[2025-10-09T06:12:02.405+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-10-09T06:12:02.483+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-09T06:12:02.827+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-09T06:12:02.959+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-10-09T06:12:02.987+0000] {subprocess.py:93} INFO - 25/10/09 06:12:02 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-10-09T06:12:03.168+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://01b28d042a8a:43671/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.170+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://01b28d042a8a:43671/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.171+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://01b28d042a8a:43671/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.195+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://01b28d042a8a:43671/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759990321001
[2025-10-09T06:12:03.203+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://01b28d042a8a:43671/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.207+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:03.211+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://01b28d042a8a:43671/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1759990321001
[2025-10-09T06:12:03.235+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://01b28d042a8a:43671/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1759990321001
[2025-10-09T06:12:03.240+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://01b28d042a8a:43671/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1759990321001
[2025-10-09T06:12:03.246+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:03.248+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://01b28d042a8a:43671/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1759990321001
[2025-10-09T06:12:03.249+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.250+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:03.290+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.291+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:03.315+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.319+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-10-09T06:12:03.367+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759990321001
[2025-10-09T06:12:03.369+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-10-09T06:12:03.386+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1759990321001
[2025-10-09T06:12:03.388+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.commons_commons-pool2-2.11.1.jar
[2025-10-09T06:12:03.407+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:03.408+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-10-09T06:12:03.650+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1759990321001
[2025-10-09T06:12:03.653+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.lz4_lz4-java-1.8.0.jar
[2025-10-09T06:12:03.696+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1759990321001
[2025-10-09T06:12:03.703+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-10-09T06:12:03.746+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1759990321001
[2025-10-09T06:12:03.751+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.slf4j_slf4j-api-2.0.7.jar
[2025-10-09T06:12:03.800+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:03.808+0000] {subprocess.py:93} INFO - 25/10/09 06:12:03 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-10-09T06:12:04.351+0000] {subprocess.py:93} INFO - 25/10/09 06:12:04 INFO SparkContext: Added file file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1759990321001
[2025-10-09T06:12:04.359+0000] {subprocess.py:93} INFO - 25/10/09 06:12:04 INFO Utils: Copying /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/commons-logging_commons-logging-1.1.3.jar
[2025-10-09T06:12:04.933+0000] {subprocess.py:93} INFO - 25/10/09 06:12:04 INFO Executor: Starting executor ID driver on host 01b28d042a8a
[2025-10-09T06:12:04.935+0000] {subprocess.py:93} INFO - 25/10/09 06:12:04 INFO Executor: OS info Linux, 6.6.87.1-microsoft-standard-WSL2, amd64
[2025-10-09T06:12:04.952+0000] {subprocess.py:93} INFO - 25/10/09 06:12:04 INFO Executor: Java version 17.0.16
[2025-10-09T06:12:05.162+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-10-09T06:12:05.171+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2904be4d for default.
[2025-10-09T06:12:05.335+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:05.467+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:05.481+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:05.612+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-10-09T06:12:05.642+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:05.776+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-10-09T06:12:05.812+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:05.823+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:05.828+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759990321001
[2025-10-09T06:12:05.841+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-10-09T06:12:05.878+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1759990321001
[2025-10-09T06:12:05.882+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.lz4_lz4-java-1.8.0.jar
[2025-10-09T06:12:05.898+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1759990321001
[2025-10-09T06:12:05.900+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.slf4j_slf4j-api-2.0.7.jar
[2025-10-09T06:12:05.917+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1759990321001
[2025-10-09T06:12:05.918+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.commons_commons-pool2-2.11.1.jar
[2025-10-09T06:12:05.938+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1759990321001
[2025-10-09T06:12:05.941+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/commons-logging_commons-logging-1.1.3.jar
[2025-10-09T06:12:05.964+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1759990321001
[2025-10-09T06:12:05.975+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Utils: /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-10-09T06:12:05.995+0000] {subprocess.py:93} INFO - 25/10/09 06:12:05 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1759990321001
[2025-10-09T06:12:06.011+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-10-09T06:12:06.027+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:06.328+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO TransportClientFactory: Successfully created connection to 01b28d042a8a/172.21.0.9:43671 after 254 ms (0 ms spent in bootstraps)
[2025-10-09T06:12:06.386+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp12827585524033038902.tmp
[2025-10-09T06:12:06.528+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp12827585524033038902.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:06.555+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
[2025-10-09T06:12:06.558+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1759990321001
[2025-10-09T06:12:06.565+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp13404321634813016022.tmp
[2025-10-09T06:12:06.783+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp13404321634813016022.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.kafka_kafka-clients-3.4.1.jar
[2025-10-09T06:12:06.864+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2025-10-09T06:12:06.877+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759990321001
[2025-10-09T06:12:06.884+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp7661484638533041165.tmp
[2025-10-09T06:12:06.885+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp7661484638533041165.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/com.google.code.findbugs_jsr305-3.0.0.jar
[2025-10-09T06:12:06.943+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2025-10-09T06:12:06.960+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1759990321001
[2025-10-09T06:12:06.975+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp11191993539172262247.tmp
[2025-10-09T06:12:06.997+0000] {subprocess.py:93} INFO - 25/10/09 06:12:06 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp11191993539172262247.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.commons_commons-pool2-2.11.1.jar
[2025-10-09T06:12:07.036+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2025-10-09T06:12:07.052+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1759990321001
[2025-10-09T06:12:07.070+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp7606946437049549700.tmp
[2025-10-09T06:12:07.104+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp7606946437049549700.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
[2025-10-09T06:12:07.404+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
[2025-10-09T06:12:07.411+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1759990321001
[2025-10-09T06:12:07.413+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp13549628903287224098.tmp
[2025-10-09T06:12:07.534+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp13549628903287224098.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2025-10-09T06:12:07.606+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2025-10-09T06:12:07.615+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:07.623+0000] {subprocess.py:93} INFO - 25/10/09 06:12:07 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp1036691785824417564.tmp
[2025-10-09T06:12:08.486+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp1036691785824417564.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2025-10-09T06:12:08.507+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2025-10-09T06:12:08.512+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1759990321001
[2025-10-09T06:12:08.516+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp618236604358366403.tmp
[2025-10-09T06:12:08.517+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp618236604358366403.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.slf4j_slf4j-api-2.0.7.jar
[2025-10-09T06:12:08.523+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2025-10-09T06:12:08.528+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1759990321001
[2025-10-09T06:12:08.531+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp17286505528846267459.tmp
[2025-10-09T06:12:08.915+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp17286505528846267459.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2025-10-09T06:12:08.933+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2025-10-09T06:12:08.935+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1759990321001
[2025-10-09T06:12:08.937+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp2039631105093768242.tmp
[2025-10-09T06:12:08.951+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp2039631105093768242.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.lz4_lz4-java-1.8.0.jar
[2025-10-09T06:12:08.964+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/org.lz4_lz4-java-1.8.0.jar to class loader default
[2025-10-09T06:12:08.965+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Fetching spark://01b28d042a8a:43671/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1759990321001
[2025-10-09T06:12:08.966+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: Fetching spark://01b28d042a8a:43671/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp14215162466913716882.tmp
[2025-10-09T06:12:08.980+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Utils: /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/fetchFileTemp14215162466913716882.tmp has been previously copied to /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/commons-logging_commons-logging-1.1.3.jar
[2025-10-09T06:12:08.997+0000] {subprocess.py:93} INFO - 25/10/09 06:12:08 INFO Executor: Adding file:/tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/userFiles-0a0d2309-bf60-40d2-a82c-dcb50d55dfdf/commons-logging_commons-logging-1.1.3.jar to class loader default
[2025-10-09T06:12:09.037+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34017.
[2025-10-09T06:12:09.039+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO NettyBlockTransferService: Server created on 01b28d042a8a:34017
[2025-10-09T06:12:09.055+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-09T06:12:09.071+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 01b28d042a8a, 34017, None)
[2025-10-09T06:12:09.081+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO BlockManagerMasterEndpoint: Registering block manager 01b28d042a8a:34017 with 434.4 MiB RAM, BlockManagerId(driver, 01b28d042a8a, 34017, None)
[2025-10-09T06:12:09.093+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 01b28d042a8a, 34017, None)
[2025-10-09T06:12:09.109+0000] {subprocess.py:93} INFO - 25/10/09 06:12:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 01b28d042a8a, 34017, None)
[2025-10-09T06:12:10.763+0000] {subprocess.py:93} INFO - 25/10/09 06:12:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-09T06:12:10.771+0000] {subprocess.py:93} INFO - 25/10/09 06:12:10 INFO SharedState: Warehouse path is 'file:/tmp/***tmpj8kq7dqp/spark-warehouse'.
[2025-10-09T06:12:17.793+0000] {subprocess.py:93} INFO - 25/10/09 06:12:17 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-10-09T06:12:17.866+0000] {subprocess.py:93} INFO - 25/10/09 06:12:17 INFO ResolveWriteToStream: Checkpoint root /opt/***/checkpoints/orders_weather/kpi resolved to file:/opt/***/checkpoints/orders_weather/kpi.
[2025-10-09T06:12:17.871+0000] {subprocess.py:93} INFO - 25/10/09 06:12:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-10-09T06:12:18.088+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO MicroBatchExecution: Starting [id = 8c50bf01-a365-4e80-a390-cc640f6a9fc8, runId = cc75b5c4-e26e-42b9-9ad1-17eff8153ca3]. Use file:/opt/***/checkpoints/orders_weather/kpi to store the query checkpoint.
[2025-10-09T06:12:18.115+0000] {subprocess.py:93} INFO - Model not found at /opt/models/revenue_model
[2025-10-09T06:12:18.121+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@48485db0] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2ccbb486]
[2025-10-09T06:12:18.200+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO OffsetSeqLog: BatchIds found from listing: 0
[2025-10-09T06:12:18.221+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO OffsetSeqLog: Getting latest batch 0
[2025-10-09T06:12:18.266+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO OffsetSeqLog: BatchIds found from listing: 0
[2025-10-09T06:12:18.272+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO OffsetSeqLog: Getting latest batch 0
[2025-10-09T06:12:18.289+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO CommitLog: BatchIds found from listing: 0
[2025-10-09T06:12:18.293+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO CommitLog: Getting latest batch 0
[2025-10-09T06:12:18.300+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO MicroBatchExecution: Resuming at batch 1 with committed offsets {KafkaV2[Subscribe[orders, weather]]: {"orders":{"0":112},"weather":{"0":4}}} and available offsets {KafkaV2[Subscribe[orders, weather]]: {"orders":{"0":112},"weather":{"0":4}}}
[2025-10-09T06:12:18.302+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[orders, weather]]: {"orders":{"0":112},"weather":{"0":4}}}
[2025-10-09T06:12:18.404+0000] {subprocess.py:93} INFO - 25/10/09 06:12:18 INFO AdminClientConfig: AdminClientConfig values:
[2025-10-09T06:12:18.406+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-10-09T06:12:18.407+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-10-09T06:12:18.408+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-10-09T06:12:18.408+0000] {subprocess.py:93} INFO - 	client.id =
[2025-10-09T06:12:18.409+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-10-09T06:12:18.410+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-10-09T06:12:18.411+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-10-09T06:12:18.412+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-10-09T06:12:18.413+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-10-09T06:12:18.414+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-10-09T06:12:18.415+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-10-09T06:12:18.415+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-10-09T06:12:18.416+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-10-09T06:12:18.417+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-10-09T06:12:18.418+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-10-09T06:12:18.419+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-10-09T06:12:18.420+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-10-09T06:12:18.421+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-10-09T06:12:18.422+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-10-09T06:12:18.424+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-10-09T06:12:18.425+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-10-09T06:12:18.426+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-10-09T06:12:18.427+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-10-09T06:12:18.428+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-10-09T06:12:18.429+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-10-09T06:12:18.430+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-10-09T06:12:18.431+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-10-09T06:12:18.432+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-10-09T06:12:18.433+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-10-09T06:12:18.434+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-10-09T06:12:18.435+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-10-09T06:12:18.436+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-10-09T06:12:18.436+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-10-09T06:12:18.437+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-10-09T06:12:18.438+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-10-09T06:12:18.438+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-10-09T06:12:18.439+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-10-09T06:12:18.439+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-10-09T06:12:18.440+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-10-09T06:12:18.441+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-10-09T06:12:18.442+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-10-09T06:12:18.443+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-10-09T06:12:18.443+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-10-09T06:12:18.444+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-10-09T06:12:18.445+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-10-09T06:12:18.446+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-10-09T06:12:18.446+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-10-09T06:12:18.447+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-10-09T06:12:18.448+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-10-09T06:12:18.449+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-10-09T06:12:18.450+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-10-09T06:12:18.450+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-10-09T06:12:18.451+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-10-09T06:12:18.452+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-10-09T06:12:18.453+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-10-09T06:12:18.454+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-10-09T06:12:18.455+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-10-09T06:12:18.456+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-10-09T06:12:18.457+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-10-09T06:12:18.458+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-10-09T06:12:18.459+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-10-09T06:12:18.459+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-10-09T06:12:18.460+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-10-09T06:12:18.461+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-10-09T06:12:18.462+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-10-09T06:12:18.463+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-10-09T06:12:18.464+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-10-09T06:12:18.465+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-10-09T06:12:18.466+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-10-09T06:12:18.467+0000] {subprocess.py:93} INFO - 
[2025-10-09T06:12:19.779+0000] {subprocess.py:93} INFO - 25/10/09 06:12:19 WARN ClientUtils: Couldn't resolve server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka
[2025-10-09T06:12:19.781+0000] {subprocess.py:93} INFO - 25/10/09 06:12:19 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-10-09T06:12:19.782+0000] {subprocess.py:93} INFO - org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient
[2025-10-09T06:12:19.783+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:551)
[2025-10-09T06:12:19.783+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.Admin.create(Admin.java:144)
[2025-10-09T06:12:19.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)
[2025-10-09T06:12:19.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)
[2025-10-09T06:12:19.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)
[2025-10-09T06:12:19.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)
[2025-10-09T06:12:19.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
[2025-10-09T06:12:19.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-10-09T06:12:19.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-10-09T06:12:19.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-10-09T06:12:19.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-10-09T06:12:19.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-10-09T06:12:19.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:19.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:19.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:19.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-10-09T06:12:19.794+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-10-09T06:12:19.795+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-10-09T06:12:19.796+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-10-09T06:12:19.796+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-10-09T06:12:19.797+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-10-09T06:12:19.798+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-10-09T06:12:19.798+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-10-09T06:12:19.799+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-10-09T06:12:19.800+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-10-09T06:12:19.801+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-10-09T06:12:19.801+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-10-09T06:12:19.802+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-10-09T06:12:19.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-10-09T06:12:19.804+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-10-09T06:12:19.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-10-09T06:12:19.805+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:19.806+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:19.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:19.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:19.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-10-09T06:12:19.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-10-09T06:12:19.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-10-09T06:12:19.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-10-09T06:12:19.811+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:19.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-09T06:12:19.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-10-09T06:12:19.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-10-09T06:12:19.814+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:19.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-10-09T06:12:19.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-10-09T06:12:19.818+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
[2025-10-09T06:12:19.819+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)
[2025-10-09T06:12:19.820+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)
[2025-10-09T06:12:19.821+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:508)
[2025-10-09T06:12:19.821+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-10-09T06:12:20.782+0000] {subprocess.py:93} INFO - 25/10/09 06:12:20 INFO AdminClientConfig: AdminClientConfig values:
[2025-10-09T06:12:20.785+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-10-09T06:12:20.789+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-10-09T06:12:20.793+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-10-09T06:12:20.794+0000] {subprocess.py:93} INFO - 	client.id =
[2025-10-09T06:12:20.795+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-10-09T06:12:20.796+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-10-09T06:12:20.797+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-10-09T06:12:20.798+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-10-09T06:12:20.799+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-10-09T06:12:20.800+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-10-09T06:12:20.802+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-10-09T06:12:20.803+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-10-09T06:12:20.804+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-10-09T06:12:20.805+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-10-09T06:12:20.806+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-10-09T06:12:20.808+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-10-09T06:12:20.809+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-10-09T06:12:20.810+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-10-09T06:12:20.810+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-10-09T06:12:20.811+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-10-09T06:12:20.812+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-10-09T06:12:20.813+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-10-09T06:12:20.813+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-10-09T06:12:20.814+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-10-09T06:12:20.815+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-10-09T06:12:20.816+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-10-09T06:12:20.816+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-10-09T06:12:20.817+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-10-09T06:12:20.818+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-10-09T06:12:20.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-10-09T06:12:20.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-10-09T06:12:20.820+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-10-09T06:12:20.821+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-10-09T06:12:20.822+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-10-09T06:12:20.823+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-10-09T06:12:20.824+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-10-09T06:12:20.825+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-10-09T06:12:20.826+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-10-09T06:12:20.827+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-10-09T06:12:20.828+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-10-09T06:12:20.829+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-10-09T06:12:20.829+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-10-09T06:12:20.830+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-10-09T06:12:20.831+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-10-09T06:12:20.832+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-10-09T06:12:20.833+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-10-09T06:12:20.834+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-10-09T06:12:20.836+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-10-09T06:12:20.837+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-10-09T06:12:20.839+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-10-09T06:12:20.845+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-10-09T06:12:20.846+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-10-09T06:12:20.847+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-10-09T06:12:20.850+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-10-09T06:12:20.855+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-10-09T06:12:20.856+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-10-09T06:12:20.857+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-10-09T06:12:20.859+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-10-09T06:12:20.860+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-10-09T06:12:20.862+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-10-09T06:12:20.863+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-10-09T06:12:20.864+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-10-09T06:12:20.865+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-10-09T06:12:20.866+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-10-09T06:12:20.868+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-10-09T06:12:20.869+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-10-09T06:12:20.871+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-10-09T06:12:20.873+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-10-09T06:12:20.874+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-10-09T06:12:20.876+0000] {subprocess.py:93} INFO - 
[2025-10-09T06:12:20.879+0000] {subprocess.py:93} INFO - 25/10/09 06:12:20 WARN ClientUtils: Couldn't resolve server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka
[2025-10-09T06:12:20.882+0000] {subprocess.py:93} INFO - 25/10/09 06:12:20 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets:
[2025-10-09T06:12:20.885+0000] {subprocess.py:93} INFO - org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient
[2025-10-09T06:12:20.886+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:551)
[2025-10-09T06:12:20.888+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.Admin.create(Admin.java:144)
[2025-10-09T06:12:20.891+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)
[2025-10-09T06:12:20.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)
[2025-10-09T06:12:20.896+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)
[2025-10-09T06:12:20.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)
[2025-10-09T06:12:20.898+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
[2025-10-09T06:12:20.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-10-09T06:12:20.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-10-09T06:12:20.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-10-09T06:12:20.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-10-09T06:12:20.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-10-09T06:12:20.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:20.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:20.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:20.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-10-09T06:12:20.911+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-10-09T06:12:20.912+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-10-09T06:12:20.913+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-10-09T06:12:20.914+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-10-09T06:12:20.915+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-10-09T06:12:20.917+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-10-09T06:12:20.919+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-10-09T06:12:20.921+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-10-09T06:12:20.923+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-10-09T06:12:20.925+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-10-09T06:12:20.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-10-09T06:12:20.927+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-10-09T06:12:20.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-10-09T06:12:20.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-10-09T06:12:20.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-10-09T06:12:20.932+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:20.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:20.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:20.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:20.936+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-10-09T06:12:20.937+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-10-09T06:12:20.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-10-09T06:12:20.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-10-09T06:12:20.940+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:20.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-09T06:12:20.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-10-09T06:12:20.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-10-09T06:12:20.946+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:20.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-10-09T06:12:20.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-10-09T06:12:20.949+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
[2025-10-09T06:12:20.950+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)
[2025-10-09T06:12:20.951+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)
[2025-10-09T06:12:20.952+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:508)
[2025-10-09T06:12:20.953+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-10-09T06:12:21.785+0000] {subprocess.py:93} INFO - 25/10/09 06:12:21 INFO AdminClientConfig: AdminClientConfig values:
[2025-10-09T06:12:21.786+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-10-09T06:12:21.787+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-10-09T06:12:21.788+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-10-09T06:12:21.789+0000] {subprocess.py:93} INFO - 	client.id =
[2025-10-09T06:12:21.790+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-10-09T06:12:21.791+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-10-09T06:12:21.792+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-10-09T06:12:21.793+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-10-09T06:12:21.794+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-10-09T06:12:21.795+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-10-09T06:12:21.795+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-10-09T06:12:21.796+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-10-09T06:12:21.797+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-10-09T06:12:21.798+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-10-09T06:12:21.799+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-10-09T06:12:21.800+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-10-09T06:12:21.801+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-10-09T06:12:21.802+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-10-09T06:12:21.802+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-10-09T06:12:21.803+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-10-09T06:12:21.804+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-10-09T06:12:21.804+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-10-09T06:12:21.805+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-10-09T06:12:21.806+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-10-09T06:12:21.806+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-10-09T06:12:21.807+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-10-09T06:12:21.807+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-10-09T06:12:21.808+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-10-09T06:12:21.809+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-10-09T06:12:21.810+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-10-09T06:12:21.810+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-10-09T06:12:21.811+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-10-09T06:12:21.812+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-10-09T06:12:21.813+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-10-09T06:12:21.814+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-10-09T06:12:21.815+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-10-09T06:12:21.816+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-10-09T06:12:21.816+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-10-09T06:12:21.817+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-10-09T06:12:21.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-10-09T06:12:21.819+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-10-09T06:12:21.819+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-10-09T06:12:21.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-10-09T06:12:21.821+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-10-09T06:12:21.822+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-10-09T06:12:21.823+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-10-09T06:12:21.823+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-10-09T06:12:21.824+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-10-09T06:12:21.825+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-10-09T06:12:21.826+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-10-09T06:12:21.827+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-10-09T06:12:21.828+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-10-09T06:12:21.828+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-10-09T06:12:21.830+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-10-09T06:12:21.830+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-10-09T06:12:21.835+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-10-09T06:12:21.837+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-10-09T06:12:21.837+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-10-09T06:12:21.838+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-10-09T06:12:21.839+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-10-09T06:12:21.840+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-10-09T06:12:21.842+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-10-09T06:12:21.843+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-10-09T06:12:21.845+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-10-09T06:12:21.847+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-10-09T06:12:21.848+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-10-09T06:12:21.849+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-10-09T06:12:21.851+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-10-09T06:12:21.852+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-10-09T06:12:21.853+0000] {subprocess.py:93} INFO - 
[2025-10-09T06:12:21.853+0000] {subprocess.py:93} INFO - 25/10/09 06:12:21 WARN ClientUtils: Couldn't resolve server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka
[2025-10-09T06:12:21.854+0000] {subprocess.py:93} INFO - 25/10/09 06:12:21 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets:
[2025-10-09T06:12:21.855+0000] {subprocess.py:93} INFO - org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient
[2025-10-09T06:12:21.856+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:551)
[2025-10-09T06:12:21.856+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.Admin.create(Admin.java:144)
[2025-10-09T06:12:21.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)
[2025-10-09T06:12:21.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)
[2025-10-09T06:12:21.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)
[2025-10-09T06:12:21.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)
[2025-10-09T06:12:21.860+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
[2025-10-09T06:12:21.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-10-09T06:12:21.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-10-09T06:12:21.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-10-09T06:12:21.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-10-09T06:12:21.864+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-10-09T06:12:21.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:21.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:21.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:21.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-10-09T06:12:21.868+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-10-09T06:12:21.869+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-10-09T06:12:21.870+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-10-09T06:12:21.871+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-10-09T06:12:21.872+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-10-09T06:12:21.873+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-10-09T06:12:21.873+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-10-09T06:12:21.874+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-10-09T06:12:21.875+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-10-09T06:12:21.876+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-10-09T06:12:21.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-10-09T06:12:21.878+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-10-09T06:12:21.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-10-09T06:12:21.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-10-09T06:12:21.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-10-09T06:12:21.881+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:21.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:21.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:21.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:21.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-10-09T06:12:21.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-10-09T06:12:21.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-10-09T06:12:21.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-10-09T06:12:21.889+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:21.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-09T06:12:21.891+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-10-09T06:12:21.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-10-09T06:12:21.893+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:21.894+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-10-09T06:12:21.895+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-10-09T06:12:21.896+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
[2025-10-09T06:12:21.897+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)
[2025-10-09T06:12:21.898+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)
[2025-10-09T06:12:21.899+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:508)
[2025-10-09T06:12:21.900+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-10-09T06:12:23.095+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 ERROR MicroBatchExecution: Query [id = 8c50bf01-a365-4e80-a390-cc640f6a9fc8, runId = cc75b5c4-e26e-42b9-9ad1-17eff8153ca3] terminated with error
[2025-10-09T06:12:23.097+0000] {subprocess.py:93} INFO - org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient
[2025-10-09T06:12:23.099+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:551)
[2025-10-09T06:12:23.101+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.Admin.create(Admin.java:144)
[2025-10-09T06:12:23.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)
[2025-10-09T06:12:23.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)
[2025-10-09T06:12:23.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)
[2025-10-09T06:12:23.105+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)
[2025-10-09T06:12:23.106+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
[2025-10-09T06:12:23.107+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-10-09T06:12:23.108+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-10-09T06:12:23.113+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-10-09T06:12:23.114+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-10-09T06:12:23.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-10-09T06:12:23.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:23.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:23.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:23.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-10-09T06:12:23.129+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-10-09T06:12:23.130+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-10-09T06:12:23.132+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-10-09T06:12:23.133+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-10-09T06:12:23.134+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-10-09T06:12:23.135+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-10-09T06:12:23.135+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-10-09T06:12:23.137+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-10-09T06:12:23.137+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-10-09T06:12:23.138+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-10-09T06:12:23.139+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-10-09T06:12:23.140+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-10-09T06:12:23.141+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-10-09T06:12:23.143+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-10-09T06:12:23.143+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-10-09T06:12:23.144+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:23.145+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-10-09T06:12:23.147+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-10-09T06:12:23.148+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-10-09T06:12:23.149+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-10-09T06:12:23.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-10-09T06:12:23.153+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-10-09T06:12:23.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-10-09T06:12:23.155+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:23.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-09T06:12:23.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-10-09T06:12:23.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-10-09T06:12:23.162+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-10-09T06:12:23.163+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-10-09T06:12:23.165+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-10-09T06:12:23.166+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
[2025-10-09T06:12:23.167+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)
[2025-10-09T06:12:23.168+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)
[2025-10-09T06:12:23.168+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:508)
[2025-10-09T06:12:23.170+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-10-09T06:12:23.171+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO MicroBatchExecution: Async log purge executor pool for query [id = 8c50bf01-a365-4e80-a390-cc640f6a9fc8, runId = cc75b5c4-e26e-42b9-9ad1-17eff8153ca3] has been shutdown
[2025-10-09T06:12:23.254+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-10-09T06:12:23.255+0000] {subprocess.py:93} INFO -   File "/opt/***/dags/spark_streaming.py", line 133, in <module>
[2025-10-09T06:12:23.257+0000] {subprocess.py:93} INFO -     query_kpi.awaitTermination()
[2025-10-09T06:12:23.258+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2025-10-09T06:12:23.258+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-10-09T06:12:23.259+0000] {subprocess.py:93} INFO -   File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-10-09T06:12:23.267+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 8c50bf01-a365-4e80-a390-cc640f6a9fc8, runId = cc75b5c4-e26e-42b9-9ad1-17eff8153ca3] terminated with exception: Failed to create new KafkaAdminClient
[2025-10-09T06:12:23.414+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO SparkContext: Invoking stop() from shutdown hook
[2025-10-09T06:12:23.415+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-09T06:12:23.427+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO SparkUI: Stopped Spark web UI at http://01b28d042a8a:4041
[2025-10-09T06:12:23.448+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-09T06:12:23.464+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO MemoryStore: MemoryStore cleared
[2025-10-09T06:12:23.465+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO BlockManager: BlockManager stopped
[2025-10-09T06:12:23.471+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-09T06:12:23.475+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-09T06:12:23.501+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO SparkContext: Successfully stopped SparkContext
[2025-10-09T06:12:23.502+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO ShutdownHookManager: Shutdown hook called
[2025-10-09T06:12:23.502+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8/pyspark-955e9e79-20cf-46ff-817f-73b68fce4187
[2025-10-09T06:12:23.506+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-966877bb-0183-4dba-85d7-4589bb967c4d
[2025-10-09T06:12:23.509+0000] {subprocess.py:93} INFO - 25/10/09 06:12:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-05c82715-6ff6-4850-bd99-c2082e89f5b8
[2025-10-09T06:12:23.578+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-10-09T06:12:23.579+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-10-09T06:12:23.596+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-10-09T06:12:23.603+0000] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=restaurant_analytics, task_id=run_spark_stream, run_id=scheduled__2025-10-09T04:00:00+00:00, execution_date=20251009T040000, start_date=20251009T061146, end_date=20251009T061223
[2025-10-09T06:12:23.635+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/utils/email.py:154: RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
  send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

[2025-10-09T06:12:23.637+0000] {configuration.py:1053} WARNING - section/key [smtp/smtp_user] not found in config
[2025-10-09T06:12:23.637+0000] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-09T06:12:23.647+0000] {configuration.py:1053} WARNING - section/key [smtp/smtp_user] not found in config
[2025-10-09T06:12:23.648+0000] {email.py:271} INFO - Email alerting: attempt 1
[2025-10-09T06:12:23.649+0000] {taskinstance.py:879} ERROR - Failed to send email to: ['quantruong1918@gmail.com']
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/socket.py", line 853, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.12/socket.py", line 838, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/socket.py", line 853, in create_connection
    raise exceptions[0]
  File "/usr/local/lib/python3.12/socket.py", line 838, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
[2025-10-09T06:12:23.671+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 249 for task run_spark_stream (Bash command failed. The command returned a non-zero exit code 1.; 807)
[2025-10-09T06:12:23.694+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 1
[2025-10-09T06:12:23.705+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
